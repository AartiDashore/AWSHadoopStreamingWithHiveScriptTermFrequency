# AWS Hadoop Streaming With Hive Script Term Frequency

## Overview: Term Frequency Hadoop Project Description:

This project runs a Hadoop Streaming pipeline that calculates term frequency statistics across a set of textcorpora. It involves two MapReduce jobs (2 Mappers and 2 Reducers python files), three shell scripts and a Hive query for further processing.

Term frequency is a triple (document_id, term, frequency_pct) that is computed over a corpus of text documents (books, web pages, etc.). In this homework, the documents will be the files in the folder textcorpora” (on VM), but make sure you delete the two problematic files chesterton-ball.txt and shakespeare-caesar.txt before you start the homework. Each file contains the text for a single book. You are already familiar with the concept of a “term”: taking a word (a string not containing whitespace), removing all characters except letters, and converting letters to lower case. There are many ways of converting a word to a term. The concept of “document ID” is new. For this project we will use the file name as the document ID – if the input file was /textcorpora/shakespere-hamlet.txt for example, we would use shakespere-hamlet as the document_id. Code at the end of this document will show you how a mapper can get the document ID for the line it is working on. The last element of the tuple is frequency_pct, which is the number of occurrences of the term in the document divided by the total number of terms in the document. To count the total number of terms in a document, you simply count all the words — including duplicates.

You will get to (document_id, term, frequency_pct)in three steps:
1. A map-reduce job reads the input document and produces tuples of the form (document_id, term, term_count)
2. A second map-reduce job reads the output of the first map-reduce job and produces tuples of the form (document_id, doc_count)
3. A Hive script joins these two data sets on document ID and produces tuples of the form (document_id, term, frequency_pct)

We will use streaming Hadoop, and mappers and reducers written in Python. Here is what’s new and not new about this problem:
1. The first map-reduce job from document to (document_id, term, term_count) looks like the usual word count (with term cleaning) except the mapper needs to get a document_id for the line it is processing. The code for this mapper is provided for you at the end of this document.
2. Also, for the first map-reduce job, one thing that is different from the word count you have seen is that the key for the reducer is not document_id (the first element in the tuple generated by the mapper), rather it is both document_id and term. In other words, you want Hadoop to group on two fields instead of one -- both document_id and term. You can control that in your call to Hadoop Streaming, by adding this flag to the call to Hadoop: -D stream.num.map.output.key.fields=2
3. The second map-reduce job transforms (document_id, term, term_count)tuples to (document_id, doc_count)tuples. You want the input for the second map-reduce job to be the output of the first map-reduce job.
4. In the hive script, first we create a Hive table for the output of the first map-reduce job(document_id, term, term_count)and a Hive table for the output of the second map-reduce job (document_id, doc_count). Then we join the two tables on document_id and compute term_frequency = term_count/doc_count which produce tuples of the form (document, term, frequency_pct).

## Folder Structure:

All the code will be in a folder named term-frequency. We have two shell scripts named run-term-count-doc-and-term and run-term-count-doc to run Hadoop streaming map reduce, and two folders named term-count-doc-and-term and term-count-doc, which will contain the mapper and reducer for the two map reduce jobs. We have a Hive script tf.hql that will create the Hive tables, linking them to the output of the two map reduce jobs, doing the SELECT statement to compute term frequency, and export the results to the local filesystem, with the tuples (document, term, frequency_pct). These fields are comma separated. Finally, we have a main shell script named run-term-frequency which will:
o Run the two map reduce jobs
o Run the Hive script
o Move the Hive output part files into a single text file tf.txt
o Clean up all temporary files and directories on the local filesystem and on HDFS

The Structure:
- term-count-doc-and-term/
    - term-count-doc-and-term_mapper.py
    - term-count-doc-and-term_reducer.py
- term-count-doc/
    - term-count-doc_mapper.py
    - term-count-doc_reducer.py
- textcorpora/
    - [Your text files go here]
- run-term-count-doc-and-term.sh
- run-term-count-doc.sh
- run-term-frequency.sh
- tf.hql

The folder structure is same in local as well as in hadoop hdfs as well.

## Setup Instructions:

1. Upload the textcorpora folder to HDFS:
   $ hadoop fs -put textcorpora /

2. Make sure all scripts and mapper/reducer files are present locally (on your Linux machine), and not just in HDFS:
   - term-count-doc-and-term/*.py
   - term-count-doc/*.py
   - run-term-count-doc-and-term.sh
   - run-term-count-doc.sh
   - run-term-frequency.sh
   - tf.hql

3. Give executable permissions in both hdfs as well as in local files:
   $ chmod +x run-term-count-doc-and-term.sh
   $ chmod +x run-term-count-doc.sh
   $ chmod +x run-term-frequency.sh
   $ chmod +x tf.hql
   $ chmod +x term-count-doc-and-term/*.py
   $ chmod +x term-count-doc/*.py

   for Hadoop:
   $hadoop fs -chmod +x run-term-count-doc-and-term.sh run-term-count-doc.sh run-term-frequency.sh tf.hql term-count-doc-and-term/*.py term-count-doc/*.py

## How to Run:
Run the full pipeline:
$ ./run-term-frequency.sh

This will:
- Clean up previous outputs.
- Run both MapReduce jobs.
- Execute Hive query (tf.hql).

After running your solution to generate the single text file tf.txt, run the command below:
`grep ,abhor, tf.txt > abhor.txt`
to generate a file abhor.txt. If the file abhor.txt has the following data, it means your code is correct.

## Expected Output:
- HDFS output folders:
  - /term-count-doc-and-term-output
  - /term-count-doc-output
  - /tf-output

- Results will be printed by the scripts using `hadoop fs -cat` and `grep` to display the processed results.

<img width="878" height="156" alt="image" src="https://github.com/user-attachments/assets/0e772ce4-002d-4b00-95ad-204bd6ad9efb" />


## Notes:
- The Hadoop Streaming JAR file path could varies depending upon the system. Ensure the Hadoop Streaming JAR path is correct in your scripts. My JAR file:
  /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.1.jar

  To find your own JAR file path, execute:
  $find /usr/ -name "hadoop-streaming*.jar" 2>/dev/null

- The mapper scripts use environment variables like map_input_file or mapreduce_map_input_file to get document IDs. This is already handled in the code, mntioned in '#' comments.

- Do not move the mapper/reducer files only to HDFS; they MUST exist locally when submitting the job.

- Also ensure that the 2 files shakespeare-caesar.txt and chesterton-ball.txt files are deleted before using textcorpora folder as input.

# Troubleshooting:
If you see errors like:
- "No such file or directory" for mapper files,
  --> Verify the files exist locally, not just in HDFS.

- PipeMapRed.waitOutputThreads() error:
  --> This likely means your mapper script crashed. Test it locally with:
      $ echo "test line" | ./term-count-doc-and-term/term-count-doc-and-term_mapper.py
